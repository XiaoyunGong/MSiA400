---
title: "MSiA400_lab3_SecondPart_XiaoyunGong"
author: "Xiaoyun Gong"
date: "10/31/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include = FALSE}
# install.packages("formattable")
# install.packages("smotefamily")
remotes::install_github("cran/DMwR")
library(DMwR)
#library(smotefamily)
library(formattable)
library(e1071)
library(caret)
library(dplyr)
library(SciViews)
library(knitr)

```

```{r, echo = FALSE}
include_graphics("Page1 3.jpg")
include_graphics("Page2 2.jpg")
include_graphics("Page3 2.jpg")
include_graphics("Page4 2.jpg")
include_graphics("Page5.jpg")
```


### Question 2 part (a) second part

##### **Implementation of getting the posterior of middle hidden states**

```{r}
xn = c(4, 4, 5, 2, 2, 4, 6, 6, 1, 4, 1, 1, 3, 5, 5, 2, 5, 4, 2, 1)
p_bias = c(2/13, 2/13, 1/13, 4/13, 2/13, 2/13)
len = length(xn)
post = rep(NA, 0)
```

```{r}
for (i in seq(2, 19, 1)) {
  x = xn[i]
  ##\pi_{n-1} = fair, \pi_{n+1} = fair
  ff = (0.75*0.75*(1/6)*0.5)/((0.75*0.75*(1/6)*0.5) + (0.25 * 0.25 * p_bias[x] * 0.5))
  post = append(post, ff)
  
  ##\pi_{n-1} = fair, \pi_{n+1} = bias
  fb = (0.75*0.25*(1/6)*0.5)/((0.75*0.25*(1/6)*0.5) + (0.75 * 0.25 * p_bias[x] * 0.5))
  post = append(post, fb)
  
  ##\pi_{n-1} = bias, \pi_{n+1} = fair
  bf = (0.25*0.75*(1/6)*0.5)/((0.25*0.75*(1/6)*0.5) + (0.25 * 0.75 * p_bias[x] * 0.5))
  post = append(post, bf)
  
   ##\pi_{n-1} = bias, \pi_{n+1} = bias
  bb = (0.25*0.25*(1/6)*0.5)/((0.25*0.25*(1/6)*0.5) + (0.75 * 0.75 * p_bias[x] * 0.5))
  post = append(post, bb)
}
```

```{r}
tab <- matrix(round(post,3), ncol = 4, byrow = TRUE)
colnames(tab) <- c("pi(n-1)=pi(n+1)=fair","pi(n-1)=fair,pi(n+1)=bias","pi(n-1)=bias,pi(n+1)=fair", "pi(n-1)=pi(n+1)=bias")
rownames(tab) <- c(2:(len-1))
tab <- as.table(tab)
formattable(tab)
```


### Question 2 part(b) Gibbs Sampling
```{r}
set.seed(1009)
pdist <- function(p){return(runif(1)<p)}
initial <- rep(TRUE, 20)
pi <- c()
cumm <- data.frame()[1:20,]
```

```{r}
##gibbs sampling

for (i in seq(1, 10000, 1)){
  
  if (initial[2] == TRUE){
    pi[1] = pdist(0.619)
  }else{
    pi[1] = pdist(0.152)
  }
  
  if(initial[19] == TRUE){
    pi[20] = pdist(0.765)
  }else{
    pi[20] = pdist(0.265)
  }
  
  for (j in seq(2, 19, 1)) {
    if (initial[j - 1] == TRUE & initial[j + 1] == TRUE) {
      pi[j] = pdist(tab[j-1, 1])
    } else if (initial[j - 1] != initial[j + 1]){
      pi[j] = pdist(tab[j-1, 2])
    } else{
      pi[j] = pdist(tab[j-1, 4])
    }
  }
  initial = pi
  cumm[i] = pi
}
```

```{r}
re <- rowSums(cumm[, -500])/9500
plot(re)
```

### Question 3 part a
```{r}
set.seed(1009)
### import data
gradAdmit = read.csv('gradAdmit.csv')
n = nrow(gradAdmit) # number of samples
# hold out 20% for testing
sample = sample.int(n = n, size = floor(.2*n), replace = F)
train = gradAdmit[-sample,]
test = gradAdmit[sample,]
### there are 80 obs in test, and 320 obs in train. 
table(train$admit)
table(test$admit)
```
```{r}
admit_train = sum(train$admit)/nrow(train)
admit_test = sum(test$admit)/nrow(test)
```

In the training set, there are 98 students got admitted out of 320 students. Therefore, `r round(admit_train,4)*100`% got admitted in the training set, and `r 100 - round(admit_train,4)*100`% didn't got admitted. 

In the testing set, there are 29 students got admitted out of 80 students. Therefore, `r round(admit_test,4)*100`% got admitted in the training set, and `r 100 - round(admit_test,4)*100`% didn't got admitted.

### Question 3 part b
```{r}
svm = svm(formula =  factor(admit) ~ ., 
                  kernel = "radial",
                  cost = 9,
                  gamma = 0.16,
                  data = train)
      
pred = predict(svm, newdata = test, type='response')

table(factor(test$admit), pred)
```
```{r}
precision = 8/(8+3)
recall = 8/(8+21)
spec = 48/(48+3)
```

Precision in this case is `r precision`. Recall in this case is `r recall`. Specificity in this case is `r spec`.

### Question 3 part c

In the training set, there are 98 students got admitted and `r 320-98` students got rejected. 
We need to generate `r 320-98-98` more samples to get class balance. 


```{r}
train$admit <- factor(train$admit)
newData = SMOTE(admit~., train, perc.over=124, perc.under=0)

# combine with original training set
newTrain <- train[train$admit==0,]
newTrain <- rbind(newTrain, newData)
table(newTrain$admit)
```

After applying the SMOTE function, the new balance is about correct. 



### Question 3 part d
```{r}
svmModel = svm(formula =  factor(admit) ~ ., 
                  kernel = "radial",
                  cost = 9,
                  gamma = 0.16,
                  data = newTrain)
      
pred = predict(svmModel, newdata = test, type='response')

table(factor(test$admit), pred)
```

The new precision is `r 15/(15+24)`, the new recall is `r 15/(14+15)`, the new specificity is `r 27/(27+24)`.


Comparing to the result without SMOTE, we see that precision dropped, recall increased, and specificity dropped. 

Although precision and specificity dropped, we have great improvement in recall. 



### Question 4 part a

This is a special case of $\lambda = 1$ of question 2 from assignment 1. 
```{r}
## lambda = 1
set.seed(1009)
lambda = 1
n = 10^8
x = runif(n,0,1)
y = -ln(x)
y10pi = length(y[y>=(10*pi)])
percy10pi = y10pi/n
#result_1 = sum(g)/n
# result_1_compare = 1 / (1+lambda^2)
```

The probability of drawing a sample x ≥ 10π from the exponential distribution (with λ = 1) is `r percy10pi *100`%.


### Question 4 Part b
```{r}
realResult = 1 / (1+lambda^2)
```
From Assignment2 Question2 part b, we knew that

$$ \int_0^{\infty}e^{-x}sin\left(x\right)dx = \frac{1}{2}$$
Also, 

$$ \int_0^{10\pi}e^{-x}sin\left(x\right)dx = -\frac{1}{2e^{10\pi }}+\frac{1}{2}$$
Hence, the desired integral is 
$$\int_{10\pi}^{\infty}e^{-x}sin\left(x\right)dx = \frac{1}{2e^{10\pi }}$$ 
### Question 4 Part c

We want to find a $p^*(x)$ larger than $p(x)$ when $x\geq10\pi$ and equals $0$ when $x<10\pi$. 

Therefore, I propose to shift $p^*(x)$ to the left by $10\pi$ ($p^*(X)=e^{-x+10\pi}$) for $x\geq0$, and set it to $0$ for $x<0$.

### Question 4 Part d

```{r}
sample <- rexp(10^6, 1)

sum <- 0
for(i in sample){
    sum <- sum + sin(i) * exp(-10*pi)
}

estimate <- sum/10^6 
estimate
```

The estimate is very close to the real value. 
